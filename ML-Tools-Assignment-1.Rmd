---
title: "ML-Tools-Assignment-1"
author: "Gyongyver Kamenar (2103380)"
date: "3/14/2022"
output: pdf_document
---

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


```{r Libraries, message=FALSE, warning=FALSE}
library(ranger)
library(tidyverse)
library(caret)
library(kableExtra)
```


# Problem 1


```{r Get the data}

caravan_data <- as_tibble(ISLR::Caravan)

set.seed(20220310)
caravan_sample <- slice_sample(caravan_data, prop = 0.2)
n_obs <- nrow(caravan_sample)
test_share <- 0.2

test_indices <- sample(seq(n_obs), floor(test_share * n_obs))
caravan_test <- slice(caravan_sample, test_indices)
caravan_train <- slice(caravan_sample, -test_indices)

```


## A)
**What would be a good evaluation metric for this problem? Think about it from the perspective of the business.**

How accurately we can predict, that the caravan will purchase insurance. 100% accurately means that we predict exactly the actual case, 0% certainty means that we predict the opposite (since it binary) in each cases. Accuracy can be calculated by adding the number of correctly classified items and divide it by the total number of items. We can also add a confidence interval using the standard deviation of the accuracy measure.

## B) and C)

```{r Average as benchmark}

mean(as.numeric(caravan_train$Purchase)) %>% kable()
caravan_train %>% group_by(Purchase) %>% count()

```

The average as the basic benchmark model, we can see that the average of the training data is `r mean(as.numeric(caravan_train$Purchase))` (the labels are 1 and 2).  This means, that if we predict 1 (No insurance) for all of them, we will mistake in just 5.9% of the cases. This really simple model already results `r sum(caravan_train$Purchase=="No")/nrow(caravan_train) * 100`% accuracy.

```{r Logistic regression}

set.seed(132456)

variables <- colnames(caravan_data[,1:85])

form <- formula(paste0("Purchase ~", paste0(variables, collapse = " + ")))

ctrl <- trainControl(method = "cv", savePredictions = "final", returnResamp = "final")

logit_model <- train(
    form = form,
    method    = "glm",
    data      = caravan_train,
    family    = binomial,
    trControl = ctrl
  )

logit_model$results %>% kable()# Accuracy 
#logit_model$pred
```
I trained a logit model with all variables, and it has `r logit_model$results$Accuracy` accuracy, which is quite bad compared to the benchmark model. So let's see the further details of the model and the predictions. 

```{r Evaluate}

# Train evaluation
confusionMatrix(logit_model$pred$pred,logit_model$pred$obs)$table

# Test evaluation

confusionMatrix(predict(logit_model,caravan_test),caravan_test$Purchase)
# Test Accuracy 0.931

```
On the train dataset, the logit model classified `r` 


## D)

The more serious mistake is, when we think/predict that a customer will purchase insurance but actually she/he does not. Because in this case, the insurance company will lose a lot of money they expected to have. 

## E)

```{r Factors}

summary(caravan_data)

caravan_data <-caravan_data %>% mutate_all(
  as.factor
)

set.seed(20220310)
caravan_sample <- slice_sample(caravan_data, prop = 0.2)
n_obs <- nrow(caravan_sample)
test_share <- 0.2

test_indices <- sample(seq(n_obs), floor(test_share * n_obs))
caravan_test <- slice(caravan_sample, test_indices)
caravan_train <- slice(caravan_sample, -test_indices)

# rerun logit model with factors
logit_model_factor <- train(
    form = formula(paste0("Purchase ~", paste0(variables, collapse = " + "))),
    method    = "glm",
    data      = caravan_train,
    family    = binomial
  )

logit_model_factor$results # Accurary 0.66
logit_model_factor$trainingData$.outcome

confusionMatrix(logit_model_factor$trainingData$.outcome,caravan_train$Purchase)

# Test evaluation

confusionMatrix(predict(logit_model_factor,caravan_test),caravan_test$Purchase)

# Test Accuracy 0.85


```

## F)


```{r tree}
set.seed(54321)
cart <- train(form=form,
  data=caravan_train,
  method = "rpart",
  tuneGrid= expand.grid(cp = 0.01),
  na.action = na.pass)

print(cart)
cart$metric

confusionMatrix(predict(cart,caravan_test),caravan_test$Purchase)
```


## G)


```{r random forest model}

tune_grid <- expand.grid(
  .mtry = 5, # c(5, 6, 7),
  .splitrule = "gini",
  .min.node.size = 15 # c(10, 15)
)
# By default ranger understands that the outcome is binary, 
#   thus needs to use 'gini index' to decide split rule
# getModelInfo("ranger")
set.seed(132456)
rf_model_p <- train(
   form = formula(paste0("Purchase ~", paste0(variables, collapse = " + "))),
  method = "ranger",
  data = caravan_train,
  tuneGrid = tune_grid,
  metric="Accuracy"
)
rf_model_p$results %>% kable() # Accuracy 0.94


confusionMatrix(predict(rf_model_p,caravan_train),caravan_train$Purchase)


# Test evaluation
confusionMatrix(predict(rf_model_p,caravan_test),caravan_test$Purchase)

```


## H)

```{r Modelling on the full set}

caravan_full_train <- anti_join(caravan_data,caravan_test)



```


## Problem 2 


```{r Get the data 2, message=FALSE, warning=FALSE}

real_estate <- read_csv('https://raw.githubusercontent.com/divenyijanos/ceu-ml/main/data/real_estate/real_estate.csv')


```






