---
title: "ML-Problem-Set-2"
author: "Gyongyver Kamenar (2103380)"
date: "3/7/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	fig.height = 5,
	fig.width = 7
)
```

```{r LIbraries, message=FALSE, warning=FALSE}
library(glmnet)
library(ggplot2)
library(purrr)
library(tidyverse)
library(kableExtra)
library(ggforce)
# My theme
devtools::source_url('https://raw.githubusercontent.com/gyongyver-droid/ceu-data-analysis/master/Assignment1/theme_gyongyver.R')
theme_set(theme_gyongyver())
```


# Problem 1

## A)
Show that the solution to this problem is given by $\hat \beta^{ridge}_0 = \sum_{i=1}^n Y_i / (n + \lambda)$. Compare this to the OLS estimator.

To minimize the expression we have to take the derivative and set it equal to 0. 

$$ \sum_{i=1}^n 2 * (Y_i -  b)*(-1) + 2 \lambda b = 0   $$
Transform to
$$ -2 \sum_{i=1}^n  (Y_i -  b)+ 2 \lambda b = 0   $$
Divide by 2
$$ - \sum_{i=1}^n  (Y_i -  b)+  \lambda b = 0   $$

Divide the summa into 2 parts. Only the Y part contains $i$ and the $b$ is taken n times.

$$ -[ \sum_{i=1}^n  (Y_i ) -  nb ]+ \lambda b = 0   $$

Reorganize the sides:
$$ nb + \lambda b = \sum_{i=1}^n  (Y_i )   $$
$$ (n + \lambda) b = \sum_{i=1}^n  (Y_i )   $$
Divide by $n+\lambda$

$$ (n + \lambda) b = \sum_{i=1}^n  (Y_i )   $$
$$b = \sum_{i=1}^n  (Y_i ) / (n + \lambda)  $$
Which is the solution of the problem:
$$\hat \beta^{ridge}_0 = \sum_{i=1}^n  (Y_i ) / (n + \lambda)  $$

Compating this to the OLS:
$$\hat \beta^{OLS}_0 = \overline Y  = \sum_{i=1}^n  (Y_i ) / n $$

So based on the two above formulas, we can see that $\hat \beta^{ridge}_0$ has $+\lambda$ in the denominator.  We know that $\lambda = 0$ in the Ridge regression so the $\hat \beta^{ridge}_0$ coefficient will be smaller than the OLS coefficient. The higher the $\lambda$ (penalty term) the higher the denominator so the ridge coefficient will be smaller. So we can see that $\lambda$ is really a penalty / shrinkage parameter.


## b)


```{r message=FALSE, warning=FALSE, paged.print=TRUE}

simulate_ridge<-function(n=10,sd=2){
  n=n
  e <- rnorm(n=n,mean=0,sd=sd)
  beta<-matrix(1,nrow = n,ncol = 1)
  y <- beta + e 

  lambda <-seq(0,20,0.1)
  beta_hat <-sum(y)/(n+lambda)
  data.frame(lambda,beta_hat, beta=1,y_hat=beta_hat+e)
}

simulate_ridge() %>% kable()



ggplot(simulate_ridge())+
  geom_line(aes(x=lambda,y=beta_hat))+
  labs(title = "Estimated Beta of Ridge regression with respect to lambda",y="Beta hat", x="Lambda")

```


## C)

Repeat part b) 1000 times, for each value of lambda compute bias, variance and MSE of $\hat \beta^{ridge}_0$ .

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
library(purrr)
df_1 <- map_df(seq(1,1000,1), ~{
  results = simulate_ridge(n=10)
  tibble(
    lambda = results$lambda,
    beta_hat = results$beta_hat,
    error = 1 - results$beta_hat
  )
  
  
}) %>% group_by(lambda) %>% summarise(bias=mean(error)^2, var=var(beta_hat), mse=bias+var)



```



## D)

Plot bias, variance and MSE as a function of lambda and interpret the result.

```{r}
ggplot(df_1, aes(x=lambda))+
  geom_line(aes(y=bias,color="Bias"))+
  geom_line(aes(y=var, color="Variance"))+
  geom_line(aes(y=mse, color="MSE"))+
  scale_colour_manual("", 
                      breaks = c("Bias", "Variance", "MSE"),
                      values = c("blue", "green", "red"))+
  labs(title = "Bias, variance and MSE as a function of lambda",y="Value",x="Lambda")
```

We can see, that as lambda is increasing, the bias is also increasing and the variance decreasing as we had expected based on on the theory of bias-variance tradeoff. The MSE takes U-shape as expected, so we can calculate that the lowest MSE is around lamdba = 5.


# Problem 2

## A)

$$ max_{u_1,u_2}\; Var(u_1X+u_2Y) \;\;\;  s.t.\;\; u_1^2 +u_2^2 = 1    $$

and suppose that 
$$Var(X) > Var(Y)  \;\;\; and \;\;\; Cov(X,Y) = E(XY) = 0 .$$

We can expand the variance formula:

$$ Var(u_1X+u_2Y)  = u_1^2 Var(X)+u_2^2 Var(Y)+ 2u_1u_2Cov(X,Y)  $$
and we know that the covariance is 0, so the problem is the following:

$$ max_{u_1,u_2}\; ( u_1^2 Var(X)+u_2^2 Var(Y) ) \;\;\;\;\;\;\; s.t. \;\; u_1^2 +u_2^2 = 1 \;\; and \;\; Var(X) > Var(Y)  \;\;\; and \;\;\; Cov(X,Y) = E(XY) = 0  $$

From this, it is trivial to see that $u_1^2 Var(X)+u_2^2 Var(Y)$ will be maximized if $u_1^2=1 \; and \; u_2^2=0$ because of the $Var(X) > Var(Y)$ condition. Therefore, there is no need to actually derive optimization problem. 

The first principle component vector is $(u_1,u_2) = (1,0)$.


### Illustration

```{r Illustration}
set.seed(20220307)
x<-rnorm(1000,mean=0,sd=2)
set.seed(43293)
y<-rnorm(1000,mean=0,sd=0.5)
# Covariance is almost zero
cov(x,y)

circles <- data.frame(
  x0 = 0,
  y0 = 0,
  r = 1
)

# Behold the some circles

data.frame(x,y) %>% ggplot()+
  geom_point(aes(x=x,y=y), alpha=0.7)+
  geom_circle(aes(x0 = x0, y0 = y0, r = r), data = circles, color="red", size=1)+
  geom_abline(intercept = 0,slope = c(0,1,99999999), color="blue", size=1)+
  scale_x_continuous(limits = c(-6,6), breaks = seq(-6,6,2))+
  scale_y_continuous(limits = c(-6,6), breaks = seq(-6,6,2))+
  labs(title = "Illustration",x="X",y="Y")
  
```



## B)

The problem:

$$ max_{u_1,u_2}\; Var(u_1X+u_2Y) \;\;\;  s.t.\;\; u_1^2 +u_2^2 = 1 \;\; and \;\; Var(X)=Var(Y)=1 \;\; and \;\; Cov(X,Y)=E(XY)=0 \; .$$

We can expand the vaiance formula as before and neglect the covatiance term becasue it is zero:

$$ Var(u_1X+u_2Y)  = u_1^2 Var(X)+u_2^2 Var(Y)+ 2u_1u_2Cov(X,Y)  =  u_1^2 Var(X)+u_2^2 Var(Y) \;\;.$$

We can substitute 1 instead of $Var(X)$ and $Var(Y)$ :


$$  u_1^2 Var(X)+u_2^2 Var(Y) = u_1^2 * 1 +u_2^2* 1=u_1^2 + u_2^2 $$


So the maximization problem is:

$$max_{u_1,u_2}\; ( u_1^2 + u_2^2 ) \;\;\;\; s.t. \;\;\;  u_1^2 + u_2^2 =1 \; .$$ 


So regardless of the $(u_1,u_2)$ values of the unit vector, the expression will be maximized and its value will be 1  because of the $(u_1^2 + u_2^2 =1)$ condition.  Intuitively, becasue of the equal variance, the X,Y points will form a circle around their mean, and in each direction of a unit vector, the variance will be the same. See the illustration below:


```{r}
set.seed(20220307)
x<-rnorm(1000,mean=0,sd=1)
set.seed(43293)
y<-rnorm(1000,mean=0,sd=1)
# Covariance is almost zero
cov(x,y)
circles <- data.frame(x0 = 0,y0 = 0,r = 1)

# Plot
data.frame(x,y) %>% ggplot()+
  geom_point(aes(x=x,y=y), alpha=0.7)+
  geom_circle(aes(x0 = x0, y0 = y0, r = r), data = circles, color="red", size=1.3)+
  geom_abline(intercept = 0,slope = c(0,1,99999999), color="blue", size=1, linetype="dashed")+
  scale_x_continuous(limits = c(-6,6), breaks = seq(-6,6,2))+
  scale_y_continuous(limits = c(-6,6), breaks = seq(-6,6,2))+
  labs(title = "Illustration",x="X",y="Y")
  
```



# Problem 3

## A)




