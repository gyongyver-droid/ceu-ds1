---
title: "ML-Tools-Assignment-3"
author: "Gyongyver Kamenar (2103380)"
date: "4/12/2022"
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    number_sections: no
    toc: no
    extra_dependencies: ["float"]
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


```{r Load  libraries}
library(tidyverse)
library(caret)
library(MLmetrics)
library(xgboost)
library(pROC)
```


Get the data

```{r Get the data}

df<-read.csv("./online news data/train.csv")
test_submission<-read.csv("./online news data/test.csv")
sample_submission<-read.csv("./online news data/sample_submission.csv")
myseed<-20220412

set.seed(myseed)
```


# Exporatory data analysis



```{r EDA}
str(train)

# Drop unnecessary columns: timedelta, article_id
df<-subset(df, select = -c(timedelta, article_id))

df<-df %>% mutate(
  is_popular = factor(is_popular, levels = list(1,0), labels = c('yes','no'))
)
```



### Visualization

```{r}




```



# Prepare modeling
```{r Prepare modelling}

# Train test split
ind <- sample(nrow(df),round(nrow(df)*0.85,0))
train<-df[ind,]
test<-df[-ind,]
```



# Logit model as benchmark

I trained a logit model on the train dataset with crossvalidation

```{r Logit}

vars <- names(train[,1:(length(train)-1)])

form <- formula(paste0("is_popular ~", paste0(vars, collapse = " + ")))

ctrl <- trainControl(method = "cv",
                     number=5, 
                     savePredictions = "final", 
                     returnResamp = "final", 
                     classProbs = TRUE,
                     summaryFunction = prSummary)

set.seed(myseed)

logit_model <- train(
    form = form,
    method    = "glm",
    data      = train,
    family    = binomial,
    trControl = ctrl
  )

logit_model$results # AUC 0.231
Accuracy(logit_model$pred$pred,train$is_popular) # Accuracy 0.869

confusionMatrix(logit_model$pred$pred,train$is_popular) #too much false negative
```

# Lasso model


```{r Lasso model}

lambda <- 10^seq(1, -4, length = 50)
grid <- expand.grid("alpha" = 1, lambda = lambda)

# Run LASSO
set.seed(myseed)
lasso_model <- train(form,
                      data = train,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = ctrl,
                      tuneGrid = grid)
# Check the output
lasso_model$bestTune
lasso_model$results

Accuracy(lasso_model$pred$pred,train$is_popular)


```


# Random forest


```{r Random forest}

# do 5-fold CV
#train_control <- trainControl(method = "cv",
 #                             number = 5,
  #                            verboseIter = FALSE)

# set tuning
tune_grid <- expand.grid(
  .mtry = 5, # c(5, 6, 7),
  .splitrule = "gini",
  .min.node.size = 10 # c(10, 15)
)
# By default ranger understands that the outcome is binary, 
#   thus needs to use 'gini index' to decide split rule
# getModelInfo("ranger")
set.seed(myseed)
rf_model_p <- train(
  form,
  method = "ranger",
  data = train,
  tuneGrid = tune_grid,
  trControl = ctrl
)


save( rf_model_p , file = 'rf_model_1.RData' )

rf_model_p$results



```


# XGBoost

```{r XGBoost}

xgb_grid_1 = expand.grid(
  nrounds = 1000,
  max_depth = c(2, 4, 6, 8, 10), #depth of the tree 2
  eta=c(0.5, 0.1, 0.07), #learning rate 0.07
  gamma = 0.01, # minimum loss reduction
  colsample_bytree=0.5, # variables to choose from (%)
  min_child_weight=1,
  subsample=0.5
)

xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = FALSE,
  returnData = FALSE,
  returnResamp = "final", # save losses across all models
  classProbs = TRUE,  # set to TRUE for AUC to be computed
  summaryFunction = prSummary,
  allowParallel = TRUE
)

set.seed(myseed)
xgb_train_1 = train(
  x = as.matrix(train[ ,1:(length(train)-1)]),
  y = as.matrix(train$is_popular),
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree"
)



xgb_train_1$results
xgb_train_1$bestTune


saveRDS(xgb1_raw_model,file = "xgb1_raw_model.rds")
#testmodel<-readRDS(file = "xgb1_raw_model.rds")

saveRDS(xgb_train_1,"./Kaggle models/xgb_train_1.rds")
#xgb_train_1<-readRDS("xgb_train_1.rds")

#save(xgb_train_1,file ="xgb_train_1.RData")

auc(as.numeric(test$is_popular),predict(xgb_train_1,test, type = "prob")$yes)
#predict(xgb_train_1,test, type = "prob")
Accuracy(predict(xgb_train_1,test),test$is_popular)
```

```{r Predict test with xgboost}

test_submission<-subset(test_submission, select = -c(timedelta, article_id))


sample_submission$score<-predict(xgb_train_1,test_submission,type="prob")$yes

write.csv(sample_submission,"sample_submission1.csv", row.names=FALSE)


```

# Train Xgb on full sample

```{r full sample xgboost}

xgb_grid_2 = expand.grid(
  nrounds = 1000,
  max_depth = 2, #depth of the tree 2
  eta=0.07, #learning rate 0.07
  gamma = 0.01, # minimum loss reduction
  colsample_bytree=0.5, # variables to choose from (%)
  min_child_weight=1,
  subsample=0.5
)

xgb_trcontrol_2 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "final", # save losses across all models
  classProbs = TRUE,  # set to TRUE for AUC to be computed
  summaryFunction = prSummary,
  allowParallel = TRUE
)

set.seed(myseed)
xgb_train_full = caret::train(
  x = as.matrix(df[ ,1:(length(df)-1)]),
  y = as.matrix(df[,length(df)]),
  trControl = xgb_trcontrol_2,
  tuneGrid = xgb_grid_2,
  method = "xgbTree"
)


xgb_train_full$results
saveRDS(xgb_train_full,"./Kaggle models/xgb_train_full.rds")
sample_submission$score<-predict(xgb_train_1,test_submission,type="prob")$yes

write.csv(sample_submission,"sample_submission2.csv", row.names=FALSE)

```


```{r Tuning XGboost}
xgb_grid_3 = expand.grid(
  nrounds = 1000,
  max_depth = c(2, 4), #depth of the tree 2
  eta=c(0.07, 0.04, 0.02), #learning rate 0.07
  gamma = c(0.05,0.07 ,0.1), # minimum loss reduction
  colsample_bytree=c(0.3,0.5,0.7), # variables to choose from (ratio)
  min_child_weight=c(0.5,1,2),
  subsample=0.5
)

xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "final", # save losses across all models
  classProbs = TRUE,  # set to TRUE for AUC to be computed
  summaryFunction = prSummary,
  allowParallel = TRUE
)

set.seed(myseed)
xgb_train_2 = caret::train(
  x = as.matrix(train[ ,1:(length(train)-1)]),
  y = as.matrix(train$is_popular),
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_3,
  method = "xgbTree"
)



xgb_train_2$results # AUC: 0.9382
xgb_train_2$bestTune

saveRDS(xgb_train_2,"./Kaggle models/xgb_train_2.rds")
#xgb_train_1<-readRDS("xgb_train_1.rds")

```

```{r Evaluate xgb_train_2}
# Test 
auc(as.numeric(test$is_popular),predict(xgb_train_2,test, type = "prob")$yes) # 0.7521
#predict(xgb_train_1,test, type = "prob")
Accuracy(predict(xgb_train_2,test),test$is_popular) # 0.8704


# Predict submission sample
sample_submission$score<-predict(xgb_train_2,test_submission,type="prob")$yes

write.csv(sample_submission,"sample_submission3.csv", row.names=FALSE)

```


```{r Train xgb2 best tune on full sample}

xgb_grid_3 = expand.grid(
  nrounds = 1000,
  max_depth = 4, #depth of the tree 2
  eta= 0.02, #learning rate 0.07
  gamma = 0.07, # minimum loss reduction
  colsample_bytree=0.7, # variables to choose from (ratio)
  min_child_weight=0.5,
  subsample=0.5
)


set.seed(myseed)
xgb_train_full2 = caret::train(
  x = as.matrix(df[ ,1:(length(df)-1)]),
  y = as.matrix(df[,length(df)]),
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_3,
  method = "xgbTree"
)


saveRDS(xgb_train_full2,"./Kaggle models/xgb_train_full2.rds")

xgb_train_full2$results # AUC 0.940977

# Test
auc(as.numeric(test$is_popular),predict(xgb_train_full2,test, type = "prob")$yes) # AUC 0.8814

# Predict submission sample
sample_submission$score<-predict(xgb_train_full2,test_submission,type="prob")$yes
write.csv(sample_submission,"./Kaggle submission/sample_submission4.csv", row.names=FALSE)
```



# Deeplearning with keras

```{r Keras modelling}
library(keras)
# Prepare data
set.seed(myseed)

keras_ind <- sample(nrow(train),round(nrow(train)*0.85,0))
keras_train<-train[keras_ind,]
keras_valid<-train[-keras_ind,]
train_x<-as.matrix(subset(keras_train,select = -is_popular))
train_y<-as.numeric(keras_train$is_popular)-1

valid_x<-as.matrix(subset(keras_valid,select = -is_popular))
valid_y<-as.numeric(keras_valid$is_popular)-1

test_x<-subset(test,select = -is_popular)
test_y<-test$is_popular

# Reshape

train_y<-to_categorical(train_y,num_classes = 2)
valid_y<-to_categorical(valid_y,num_classes = 2)

# Build model

simple_keras <- keras_model_sequential()
simple_keras |>
    layer_dense(units = 32, activation = 'relu', input_shape = c(58)) |>
    layer_dropout(rate = 0.2) |>
    layer_dense(units = 2, activation = 'softmax')

summary(simple_keras)
```

```{r Train simple keras model}
library(tensorflow)
batch_size=16
compile(
    simple_keras,
    loss = 'binary_crossentropy',
    optimizer = optimizer_adam(),
    metrics = tf$keras$metrics$AUC()
)

history1 <-fit(
    simple_keras,
    train_x, train_y,
    epochs = 10,
    batch_size = 16,
    #steps_per_epoch =floor(nrow(train_x)*0.85/batch_size), 
    validation_split = 0.2,
    #validation_steps =floor(nrow(train_x)*0.15/batch_size) 
)

simple_keras %>% save_model_hdf5("./Kaggle models/simple_keras.h5")

plot(history1)

```


```{r Keras model 2}

keras_model_2 <- keras_model_sequential()
keras_model_2 |>
    layer_dense(units = 64, activation = 'relu', input_shape = c(58)) |>
    layer_dense(units = 128, activation = 'relu', input_shape = c(58)) |>
    layer_dropout(rate = 0.22) |>
    layer_dense(units = 2, activation = 'softmax')

summary(keras_model_2)



compile(
    keras_model_2,
    loss = 'binary_crossentropy',
    optimizer = optimizer_adam(),
    metrics = tf$keras$metrics$AUC()
)

set.seed(myseed)
batch_size=32

history2 <-fit(
    keras_model_2,
    train_x, train_y,
    epochs = 10,
    batch_size = batch_size,
    steps_per_epoch =floor(nrow(train_x)*0.8/batch_size), 
    #validation_data = list(valid_x,valid_y),
    validation_split = 0.2,
    validation_steps =floor(nrow(train_x)*0.2/batch_size) 
)

keras_model_2 %>% save_model_hdf5("./Kaggle models/keras_model_2.h5")

plot(history2)

```


# Feature engineering


```{r Feature engineering}
# convert  dummy variables to factors
df <- df %>% mutate_at(
  .vars = vars(colnames(df)[startsWith(colnames(df),"data_channel") |
                            startsWith(colnames(df),"weekday") |
                            startsWith(colnames(df),"is_week")]),
  .funs = (as.factor())
)
```


# Load models

```{r}


```

# Train XGBoost with feature engineered data




